{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f36d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# Import some useful packages for this homework\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" and \"Subset\" are possibly useful.\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fdba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dataset_root': './data/food11-hw13',\n",
    "    'save_dir': './hw13',\n",
    "    'exp_name': \"simple_baseline\",\n",
    "    'batch_size': 64,\n",
    "    'lr': 3e-4,\n",
    "    'seed': 20220013,\n",
    "    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD. See the Knowledge_Distillation part for more information.\n",
    "    'weight_decay': 5e-5,\n",
    "    'grad_norm_max': 10,\n",
    "    'n_epochs': 90,\n",
    "    'patience': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e2fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_root': './data/food11-hw13', 'save_dir': './hw13', 'exp_name': 'simple_baseline', 'batch_size': 64, 'lr': 0.0003, 'seed': 20220013, 'loss_fn_type': 'KD', 'weight_decay': 5e-05, 'grad_norm_max': 10, 'n_epochs': 90, 'patience': 300}\n"
     ]
    }
   ],
   "source": [
    "myseed = cfg['seed']  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "random.seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "save_path = os.path.join(cfg['save_dir'], cfg['exp_name']) # create saving directory\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# define simple logging functionality\n",
    "log_fw = open(f\"{save_path}/log.txt\", 'w') # open log file to save log outputs\n",
    "def log(text):     # define a logging function to trace the training process\n",
    "    print(text)\n",
    "    log_fw.write(str(text)+'\\n')\n",
    "    log_fw.flush()\n",
    "\n",
    "log(cfg)  # log your configs to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d742b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# define training/testing transforms\n",
    "test_tfm = transforms.Compose([\n",
    "    # It is not encouraged to modify this part if you are using the provided teacher model. This transform is stardard and good enough for testing.\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    # add some useful transform or augmentation here, according to your experience in HW3.\n",
    "    transforms.Resize(256),  # You can change this\n",
    "    transforms.RandomCrop(224), # You can change this, but be aware of that the given teacher model's input size is 224.\n",
    "    # The training input size of the provided teacher model is (3, 224, 224).\n",
    "    # Thus, Input size other then 224 might hurt the performance. please be careful.\n",
    "    transforms.RandomHorizontalFlip(), # You can change this.\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a1a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, tfm=test_tfm, files = None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        try:\n",
    "            label = int(fname.split(\"\\\\\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358ef7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ./data/food11-hw13\\training sample ./data/food11-hw13\\training\\0_0.jpg\n",
      "One ./data/food11-hw13\\validation sample ./data/food11-hw13\\validation\\0_0.jpg\n"
     ]
    }
   ],
   "source": [
    "train_set = FoodDataset(os.path.join(cfg['dataset_root'],\"training\"), tfm=train_tfm)\n",
    "train_loader = DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True)\n",
    "\n",
    "valid_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\n",
    "valid_loader = DataLoader(valid_set, batch_size=cfg['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b0e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example implementation of Depthwise and Pointwise Convolution \n",
    "def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels), #depthwise convolution\n",
    "        nn.Conv2d(in_channels, out_channels, 1), # pointwise convolution\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9a8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireBlock(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "        super(FireBlock, self).__init__()\n",
    "        \n",
    "        # Squeeze layer\n",
    "        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Expand layers\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.squeeze(x)\n",
    "        out = self.squeeze_activation(out)\n",
    "        expand1x1 = self.expand1x1(out)\n",
    "        expand1x1 = self.expand1x1_activation(expand1x1)\n",
    "        expand3x3 = self.expand3x3(out)\n",
    "        expand3x3 = self.expand3x3_activation(expand3x3)\n",
    "        \n",
    "        out = torch.cat([expand1x1, expand3x3], dim=1)  # 在通道维度上拼接\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc525df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 224\n",
    "f = 3\n",
    "p = 1\n",
    "s = 2\n",
    "(w-f+2*p)/s + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9807e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = dwpw_conv(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.bn(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv(out)\n",
    "        out = torch.cat((x, out), dim=1)  # 将当前输入和输出在通道维度上拼接\n",
    "        return out\n",
    "\n",
    "# 定义DenseNet网络\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, growth_rate=6, num_blocks=3, basic_block=6):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        num_channels = 64\n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.dense_blocks.append(self._make_dense_block(num_channels, growth_rate, basic_block))\n",
    "            num_channels += growth_rate*(basic_block)\n",
    "        \n",
    "        self.convdp = dwpw_conv(64+num_blocks*basic_block*growth_rate, (64+num_blocks*basic_block*growth_rate)//3, kernel_size=3, padding=1)\n",
    "        self.bn_final = nn.BatchNorm2d(64+num_blocks*basic_block*growth_rate)\n",
    "        self.bn_final_ = nn.BatchNorm2d((64+num_blocks*basic_block*growth_rate)//3)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear((64+num_blocks*basic_block*growth_rate)//3, num_classes)\n",
    "    \n",
    "    def _make_dense_block(self, in_channels, growth_rate, num_block):\n",
    "        layers = []\n",
    "        for i in range(num_block):  # 每个Dense Block包含4个Basic Block\n",
    "            layers.append(BasicBlock(in_channels, growth_rate))\n",
    "            in_channels += growth_rate\n",
    "        layers.append(nn.Conv2d(in_channels,in_channels,1,1))\n",
    "        layers.append(nn.AvgPool2d(2, 2, 0))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        for block in self.dense_blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        out = self.bn_final(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.convdp(out)\n",
    "        out = self.bn_final_(out)\n",
    "        out = F.avg_pool2d(out, kernel_size=7)  # 全局平均池化层\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525cb265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-4           [-1, 64, 56, 56]             128\n",
      "            Conv2d-5           [-1, 64, 56, 56]             640\n",
      "            Conv2d-6            [-1, 6, 56, 56]             390\n",
      "        BasicBlock-7           [-1, 70, 56, 56]               0\n",
      "       BatchNorm2d-8           [-1, 70, 56, 56]             140\n",
      "            Conv2d-9           [-1, 70, 56, 56]             700\n",
      "           Conv2d-10            [-1, 6, 56, 56]             426\n",
      "       BasicBlock-11           [-1, 76, 56, 56]               0\n",
      "      BatchNorm2d-12           [-1, 76, 56, 56]             152\n",
      "           Conv2d-13           [-1, 76, 56, 56]             760\n",
      "           Conv2d-14            [-1, 6, 56, 56]             462\n",
      "       BasicBlock-15           [-1, 82, 56, 56]               0\n",
      "      BatchNorm2d-16           [-1, 82, 56, 56]             164\n",
      "           Conv2d-17           [-1, 82, 56, 56]             820\n",
      "           Conv2d-18            [-1, 6, 56, 56]             498\n",
      "       BasicBlock-19           [-1, 88, 56, 56]               0\n",
      "      BatchNorm2d-20           [-1, 88, 56, 56]             176\n",
      "           Conv2d-21           [-1, 88, 56, 56]             880\n",
      "           Conv2d-22            [-1, 6, 56, 56]             534\n",
      "       BasicBlock-23           [-1, 94, 56, 56]               0\n",
      "      BatchNorm2d-24           [-1, 94, 56, 56]             188\n",
      "           Conv2d-25           [-1, 94, 56, 56]             940\n",
      "           Conv2d-26            [-1, 6, 56, 56]             570\n",
      "       BasicBlock-27          [-1, 100, 56, 56]               0\n",
      "           Conv2d-28          [-1, 100, 56, 56]          10,100\n",
      "        AvgPool2d-29          [-1, 100, 28, 28]               0\n",
      "      BatchNorm2d-30          [-1, 100, 28, 28]             200\n",
      "           Conv2d-31          [-1, 100, 28, 28]           1,000\n",
      "           Conv2d-32            [-1, 6, 28, 28]             606\n",
      "       BasicBlock-33          [-1, 106, 28, 28]               0\n",
      "      BatchNorm2d-34          [-1, 106, 28, 28]             212\n",
      "           Conv2d-35          [-1, 106, 28, 28]           1,060\n",
      "           Conv2d-36            [-1, 6, 28, 28]             642\n",
      "       BasicBlock-37          [-1, 112, 28, 28]               0\n",
      "      BatchNorm2d-38          [-1, 112, 28, 28]             224\n",
      "           Conv2d-39          [-1, 112, 28, 28]           1,120\n",
      "           Conv2d-40            [-1, 6, 28, 28]             678\n",
      "       BasicBlock-41          [-1, 118, 28, 28]               0\n",
      "      BatchNorm2d-42          [-1, 118, 28, 28]             236\n",
      "           Conv2d-43          [-1, 118, 28, 28]           1,180\n",
      "           Conv2d-44            [-1, 6, 28, 28]             714\n",
      "       BasicBlock-45          [-1, 124, 28, 28]               0\n",
      "      BatchNorm2d-46          [-1, 124, 28, 28]             248\n",
      "           Conv2d-47          [-1, 124, 28, 28]           1,240\n",
      "           Conv2d-48            [-1, 6, 28, 28]             750\n",
      "       BasicBlock-49          [-1, 130, 28, 28]               0\n",
      "      BatchNorm2d-50          [-1, 130, 28, 28]             260\n",
      "           Conv2d-51          [-1, 130, 28, 28]           1,300\n",
      "           Conv2d-52            [-1, 6, 28, 28]             786\n",
      "       BasicBlock-53          [-1, 136, 28, 28]               0\n",
      "           Conv2d-54          [-1, 136, 28, 28]          18,632\n",
      "        AvgPool2d-55          [-1, 136, 14, 14]               0\n",
      "      BatchNorm2d-56          [-1, 136, 14, 14]             272\n",
      "           Conv2d-57          [-1, 136, 14, 14]           1,360\n",
      "           Conv2d-58            [-1, 6, 14, 14]             822\n",
      "       BasicBlock-59          [-1, 142, 14, 14]               0\n",
      "      BatchNorm2d-60          [-1, 142, 14, 14]             284\n",
      "           Conv2d-61          [-1, 142, 14, 14]           1,420\n",
      "           Conv2d-62            [-1, 6, 14, 14]             858\n",
      "       BasicBlock-63          [-1, 148, 14, 14]               0\n",
      "      BatchNorm2d-64          [-1, 148, 14, 14]             296\n",
      "           Conv2d-65          [-1, 148, 14, 14]           1,480\n",
      "           Conv2d-66            [-1, 6, 14, 14]             894\n",
      "       BasicBlock-67          [-1, 154, 14, 14]               0\n",
      "      BatchNorm2d-68          [-1, 154, 14, 14]             308\n",
      "           Conv2d-69          [-1, 154, 14, 14]           1,540\n",
      "           Conv2d-70            [-1, 6, 14, 14]             930\n",
      "       BasicBlock-71          [-1, 160, 14, 14]               0\n",
      "      BatchNorm2d-72          [-1, 160, 14, 14]             320\n",
      "           Conv2d-73          [-1, 160, 14, 14]           1,600\n",
      "           Conv2d-74            [-1, 6, 14, 14]             966\n",
      "       BasicBlock-75          [-1, 166, 14, 14]               0\n",
      "      BatchNorm2d-76          [-1, 166, 14, 14]             332\n",
      "           Conv2d-77          [-1, 166, 14, 14]           1,660\n",
      "           Conv2d-78            [-1, 6, 14, 14]           1,002\n",
      "       BasicBlock-79          [-1, 172, 14, 14]               0\n",
      "           Conv2d-80          [-1, 172, 14, 14]          29,756\n",
      "        AvgPool2d-81            [-1, 172, 7, 7]               0\n",
      "      BatchNorm2d-82            [-1, 172, 7, 7]             344\n",
      "          Dropout-83            [-1, 172, 7, 7]               0\n",
      "           Conv2d-84            [-1, 172, 7, 7]           1,720\n",
      "           Conv2d-85             [-1, 57, 7, 7]           9,861\n",
      "      BatchNorm2d-86             [-1, 57, 7, 7]             114\n",
      "           Linear-87                   [-1, 11]             638\n",
      "================================================================\n",
      "Total params: 111,797\n",
      "Trainable params: 111,797\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 71.08\n",
      "Params size (MB): 0.43\n",
      "Estimated Total Size (MB): 72.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = f\"{save_path}/student_best.ckpt\" # the ckpt path of the best student model.\n",
    "student_model = DenseNet(in_channels=3, num_classes=11)\n",
    "# student_model.load_state_dict(torch.load(ckpt_path, map_location='cpu')) # load the state dict and set it to the student model\n",
    "summary(student_model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e888f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hasaki/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\hasaki\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hasaki\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load provided teacher model (model architecture: resnet18, num_classes=11, test-acc ~= 89.9%)\n",
    "teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n",
    "# load state dict\n",
    "teacher_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\n",
    "teacher_model.load_state_dict(torch.load(teacher_ckpt_path))\n",
    "# Now you already know the teacher model's architecture. You can take advantage of it if you want to pass the strong or boss baseline. \n",
    "# Source code of resnet in pytorch: (https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)\n",
    "# You can also see the summary of teacher model. There are 11,182,155 parameters totally in the teacher model\n",
    "# summary(teacher_model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86ee29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.5, temperature=1):\n",
    "    # Apply Softmax to student and teacher logits\n",
    "    student_probs = nn.Softmax(dim=1)(student_logits / temperature)\n",
    "    teacher_probs = nn.Softmax(dim=1)(teacher_logits / temperature)\n",
    "    \n",
    "    # Calculate the KL divergence loss\n",
    "    kd_loss = nn.KLDivLoss()(student_probs, teacher_probs)\n",
    "    \n",
    "    # Calculate the CrossEntropy loss\n",
    "    ce_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    \n",
    "    # Combine the two losses with the specified weights\n",
    "    loss = alpha * pow(temperature, 2) * kd_loss + (1 - alpha) * ce_loss\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea53c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# choose the loss function by the config\n",
    "if cfg['loss_fn_type'] == 'CE':\n",
    "    # For the classification task, we use cross-entropy as the default loss function.\n",
    "    loss_fn = nn.CrossEntropyLoss() # loss function for simple baseline.\n",
    "\n",
    "if cfg['loss_fn_type'] == 'KD': # KD stands for knowledge distillation\n",
    "    loss_fn = loss_fn_kd # implement loss_fn_kd for the report question and the medium baseline.\n",
    "\n",
    "# You can also other types of knowledge distillation techniques, but use function name other than `loss_fn_kd`\n",
    "# For example:\n",
    "# def loss_fn_custom_kd():\n",
    "#     pass\n",
    "# if cfg['loss_fn_type'] == 'custom_kd':\n",
    "#     loss_fn = loss_fn_custom_kd\n",
    "\n",
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "log(f\"device: {device}\")\n",
    "\n",
    "# The number of training epochs and patience.\n",
    "n_epochs = cfg['n_epochs']\n",
    "patience = cfg['patience'] # If no improvement in 'patience' epochs, early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447c2e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cdda0490f0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGiCAYAAAA8xWYrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMoUlEQVR4nO3de1xUZf4H8M+ZGWa4CIOA3OQi3k28gnetLKXMrLabZYHXVkrzQuuW2/5qc9u13bYyK83ylpfMLlZWrEXlekVRBO9XQEEEEZAZrgMzc35/wIyioA4C58zM5/16zWuXwxnme04mn57zPM9XEEVRBBEREZFMKKQugIiIiOhqDCdEREQkKwwnREREJCsMJ0RERCQrDCdEREQkKwwnREREJCsMJ0RERCQrDCdEREQkKwwnREREJCsMJ0RERCQrNoeT7du3Y9y4cQgODoYgCPjuu+9ueP6mTZswevRotGvXDl5eXhgyZAh+/vnnptZLREREDs7mcFJeXo4+ffrgww8/vKXzt2/fjtGjRyMxMRGpqakYOXIkxo0bh7S0NJuLJSIiIscn3E7jP0EQ8O233+KRRx6x6X09e/bE+PHj8dprrzX1o4mIiMhBqVr7A81mM0pLS+Hj49PoOQaDAQaDod57iouL4evrC0EQWqNMIiIiuk2iKKK0tBTBwcFQKG79YU2rh5N33nkH5eXlePLJJxs9Z+HChXjjjTdasSoiIiJqKTk5OQgJCbnl81v1sc6GDRswbdo0fP/99xg1alSj5107cqLT6RAWFoacnBx4eXk1tVwiIiJqRXq9HqGhoSgpKYFWq73l97XayMnGjRsxdepUfPXVVzcMJgCg0Wig0WiuO+7l5cVwQkREZGdsnZLRKvucbNiwAZMmTcLnn3+OsWPHtsZHEhERkZ2yeeSkrKwMZ86csX6dlZWF9PR0+Pj4ICwsDPPnz0dubi7WrFkDoDaYxMXF4f3338fgwYORn58PAHBzc7NpiIeIiIicg80jJ/v370e/fv3Qr18/AEBCQgL69etnXRacl5eH7Oxs6/nLli2D0WjEjBkzEBQUZH3Nnj27mS6BiIiIHMltTYhtLXq9HlqtFjqdjnNOiIiI7ERTf3+ztw4RERHJCsMJERERyQrDCREREckKwwkRERHJCsMJERERyQrDCREREckKwwkRERHJCsMJERERyQrDiZ0p0Fdhyf/OoLi8WupSiIiIWkSrdSWm5rF0WwZW7ToLUQRmjOwsdTlERETNjiMnduZkfikAoKLaKHElRERELYPhxM5kXCqTugQiIqIWxXBiR8oMRlzUGwAA8m/XSERE1DQMJ3Yk61K59f8zmxARkaNiOLEjmYVXHulw5ISIiBwVw4kdySi4Kpxw7ISIiBwUw4kdySgsv/lJREREdo7hxI5kXjXnhAMnRETkqBhO7ITZLCLr6jknEtZCRETUkhhO7MQFXSWqaszWr0XOiCUiIgfFcGIn6j3SISIicmAMJ3bi2p1hOXBCRESOiuHETlw7csJsQkREjorhxE5YNmBr56kBwJETIiJyXAwndsIyctKpnQcAbsJGRESOi+HEDpQbjMjTVQEAOrVrI3E1RERELYvhxA5k1e0M6+uhhre7CwA+1iEiIsfFcGIHLCt1OrVrAwGCxNUQERG1LIYTO5BRN9+kYzsPCHXZhJuwERGRo2I4sQOZdSMnHesmwxIRETkyhhM7YFmp09GvjfWhDsdNiIjIUTGcyJzZLFr3OOnk3waW5zp8qkNERI6K4UTm8vRVqKoxw0UpILSt21UjJ0wnRETkmBhOZM4y3yTMxx0qpeKqCbESFkVERNSCGE5k7srOsNx8jYiInAPDicxlWFfq1IYTyz4nHDghIiJHxXAic5lX7XECgI91iIjI4TGcyFzmVbvDArhqf1imEyIickwMJzJWUW3EBWvDv/ojJ0RERI6K4UTGLI90fDzU8HZX1/seH+sQEZGjYjiRscxCy86wV7atF7gJGxEROTiGExnLKKg/3+Rq3ISNiIgcFcOJjFlHTtpdPXJS+78cOSEiIkfFcCJjmdfscQJc2eeEiIjIUTGcyJTZLF61O6zHdd/nwAkRETkqhhOZytdXobLGBJVCQKiPu/U4H+sQEZGjYziRKcuoSZivO1yUV/4xsSsxERE5OoYTmcosrJtv4ld/pQ43YSMiIkdnczjZvn07xo0bh+DgYAiCgO++++6m79m2bRuioqLg6uqKjh074uOPP25KrU7FuozYv/58E+uEWA6cEBGRg7I5nJSXl6NPnz748MMPb+n8rKwsPPDAAxgxYgTS0tLwl7/8BbNmzcI333xjc7HOxLKMuJPf9XucAMwmRETkuFS2vmHMmDEYM2bMLZ//8ccfIywsDIsWLQIA9OjRA/v378d//vMfPPbYY7Z+vNO4thuxheWxTr6uCrvOFMJFqYCLUoCLUgEvVxdo3Vzg6aqCQsHnP0REZJ9sDie2Sk5ORkxMTL1j9913H1asWIGamhq4uLhc9x6DwQCDwWD9Wq/Xt3SZslJRbURuSSWA63eHVdWFjuTMIiRnFjX4fkEAPDUqeLurEeClQaDWDcFaVwRqXRGkdUOEnwfCfd3h6qJs2QshIiJqghYPJ/n5+QgICKh3LCAgAEajEYWFhQgKCrruPQsXLsQbb7zR0qXJVlbdI5227i5o61G/4d/onoHYduoSCsuqUWMyo9pkRo3JDEONGaVVRlTWmCCKgL7KCH2VEdnFFQAuX/cZCgEI9XFHp3Zt0KmdB3oEeaFXey06tmsDJUddiIhIQi0eToArzeosxLpNOq49bjF//nwkJCRYv9br9QgNDW25AmXmyiOd6+ebtPd2w6rJAxt9r8Fogr7SCF1lDUoqqpGvr0K+rgoXSqqQp6vEhZJKZF4qR6nBiHNFFThXVIHfT1x5v7taiZ7BXohsr0W/sLYYHOEDfy/XZr9GIiKixrR4OAkMDER+fn69YwUFBVCpVPD19W3wPRqNBhqNpqVLk62MS5aGf9fvDHszGpUS7TyVaOfZ+P0TRRGXygzIKChHxqUynCkow9ELOhzJ1aOi2oR9Zy9j39nLWLXrLAAgws8DgyJ8MDDCB0M7+SFQy7BCREQtp8XDyZAhQ/DDDz/UO/bLL78gOjq6wfkmdOORk+YgCAL8PV3h7+mKIZ2uBESTWUTmpTIcztXh0Hkd9p0txrE8PbIKy5FVWI4v9uUAALoHemJkd3+M7OaP/mHeUCm5XQ4RETUfm8NJWVkZzpw5Y/06KysL6enp8PHxQVhYGObPn4/c3FysWbMGABAfH48PP/wQCQkJeO6555CcnIwVK1Zgw4YNzXcVDubKBmy2j5zcDqVCQJcAT3QJ8MSj/UMAALrKGuw/W4yUrGLsySzCoVwdTuSX4kR+KZb+LwNerirc1c0fD0QG4u5u/nBTc5ItERHdHpvDyf79+zFy5Ejr15a5IRMnTsTq1auRl5eH7Oxs6/cjIiKQmJiIuXPn4qOPPkJwcDAWL17MZcSNEEWxxUdObKF1c8G9PQJwb4/aSc3F5dXYfuoStp4swLZTl1BSUYMfDl7ADwcvwF2txD3d/TG2VxCDChERNZkgivJvIafX66HVaqHT6eDl5SV1OS0qT1eJIQt/h0oh4Pjf76/XV0duTGYR6TmX8fPRi/jpUJ51+TNQO7F2TGQQnogOwaAIn0YnPxMRkeNq6u/vVlmtQ7fO2vDPx13WwQSofQwUFe6DqHAfzB/THQfP65B4OM8aVL45cB7fHDiPMB93PB4Vgkf7t0dIW/eb/2AiInJqDCcyk1m3UufanWHlThAE9A31Rt9Qb8wf0x2p5y7jq/3n8eOhC8gursC7Safw3q+ncGeXdpg0tAPu6tqOu9gSEVGDGE5kJqNu5OTanWHtiSAIiO7gg+gOPnj9oTuw5Ug+vtp/HsmZRdh26hK2nbqEMB93xA0JxxNRodC6c9UWERFdwXAiMxl2OnLSGHe1Co/2D8Gj/UNwtrAc6/acw5f7c5BdXIE3fzqO//xyEn/oF4I/3tkREa28OomIiORJ3pManJCcVuo0tw5+Hvjrg3dgz1/uxcJHe6F7oCeqaszYkJKNe975H15Yn4pD50ukLpOIiCTGkRMZqaw2Ndrwz5G4q1V4emAYnhoQir1Zxfh0eyZ+O1GAxMP5SDycj2GdfRF/VycM7+zHVT5ERE6I4URGLA3/vN1d4HNNwz9HJAgCBnf0xeCOvjiRr8eybZnYfPACdp0pwq4zRegX5o2E0V0ZUoiInAwf68iIVDvDykH3QC+8N74vts27G5OGdoCriwJp2SWIXZGC8cv2IDmjSOoSiYiolTCcyIgjzze5VSFt3fG3h3pi+59HYvKwDlCrFEg5W4ynP92DCZ/uQeq5YqlLJCKiFsZwIiNXuhE7bzix8Pd0xevjemLbvLvx7OAwuCgF7M4owmNLkxG/NtX6CIyIiBwPw4mMXBk5cb7HOo0J0rrhzUd6Yeuf7sb46FAoBGDL0XyMfncb/rb5KIrLq6UukYiImhnDiUzUNvyzjJwwnFwrpK07/vV4b2yZcydGdmsHo1nE6t1ncdfbW7FsWwaqakxSl0hERM2E4UQmLuoNKK82QakQEObDcNKYrgGeWDV5INZNHYQeQV4orTJi4X9PYPR72/Db8YtSl0dERM2A4UQmLKMmYT7uUKv4j+Vmhnfxw48vDsd/nuiDAC8NcoorMfWz/Zi6eh+yiyqkLo+IiG4DfwvKREbdBE9nXEbcVEqFgMejQvDbS3dj+p0doVII+O1EAUa9tw3vJZ3iox4iIjvFcCITGQV18038uVLHVm00Ksx/oAe2zBmBYZ19UW004/3fTmP0e9uw9USB1OUREZGNGE5kIpMjJ7ets78n1k0dhI8m9EeQ1hU5xZWYvHofZm1IQ1GZQeryiIjoFjGcyESmtRsxR05uhyAIGNs7CL8m3IXnRkRAIQCbD17AqHe3YdOB8xBFUeoSiYjoJhhOZKCq5krDP+5x0jw8NCq8OvYOfDdjGLoHeuJyRQ0SvjyIiav2IaeYE2aJiOSM4UQGsgrLIYqA1s0Fvk7Q8K819Q7xxg8vDse8+7pBrVJg+6lLiHlvOz7bfRZmM0dRiIjkiOFEBq7eGZbdd5ufi1KBGSM7Y8vsERgY4YPKGhNe33wUsSv34kLdiBUREckHw4kMWOeb+HG+SUvq2K4NvnhuMBY83BOuLgrsOlOE+xZt51wUIiKZYTiRAWvDP3/ON2lpCoWAuCEdkDhrBPqFeaO0yoiELw8ifl0qV/QQEckEw4kMXFlGzJGT1tKxXRt8NX0I5t3XDSqFgJ+PXkTMe9vx6zFugU9EJDWGE4nVNvyrDSds+Ne6VHVzUb6bMQzdAjxRVF6NaWv24/Xvj3B3WSIiCTGcSKyg1IAyg7G24Z+vu9TlOKXI9lpsfnEYpg2PAAB8lnwOf1iyG2fqdu0lIqLWxXAiMct8k9C2btColBJX47w0KiX++uAdWDVpAHw91Diep8e4D3Zi475sTpYlImplDCcSu7KMmPNN5GBkd3/8d/YIDO/sh8oaE17+5jBe3JAGfVWN1KURETkNhhOJZViXEXO+iVz4e7lizZSBePn+7lApBPx4KA9jF+/AkVyd1KURETkFhhOJWSfDshuxrCgUAp6/uxO+jB+CkLZuyCmuxKNLd2PjvmypSyMicngMJxLLLOTIiZz1D2uLn14cgVE9/FFtNOPlbw5j3lcHuZqHiKgFMZxIqKrGhPOXLQ3/OHIiV1p3F3wSG40/398NCgH4KvU8/rBkN87W7U9DRETNi+FEQmeLahv+ebmq4NeGDf/kTKEQ8MLdnbFu6qArq3k+3IlfjuZLXRoRkcNhOJHQ1St12PDPPgzt7IefZo1AVHhblFYZ8ce1qXj75xPscExE1IwYTiRkbfjHnWHtSqDWFV/8cTCmDKvdtO2jrRl4bs1+LjcmImomDCcSyrBuW8/5JvbGRanAa+PuwHvj+0CtUuC3EwX4w0e7kMV5KEREt43hREKWkRP21LFff+gXgq/jhyDQyxUZl8rx8Ic7se3UJanLIiKyawwnErm64R9X6ti33iHe2PziMPQP84a+yojJq1Lw6fZMbntPRNREDCcSuVRqQKnBCIUAhLPhn93z93TFhj8OxvjoUJhF4B+Jx5HwJfdDISJqCoYTiVjmm4T6uLPhn4PQqJR467FeeOOhnlAqBHyblosJn+5BUZlB6tKIiOwKw4lEuDOsYxIEAROHdsDaKQPh5arCgewS/GHJbpwpKJO6NCIiu8FwIhHON3FsQzv7YdMLwxDm447s4go8umQXdp8plLosIiK7wHAikQzrSh2GE0fV2b8Nvn1hKKLC20JfZUTcyhR8uS9H6rKIiGSP4UQiV0ZO+FjHkfm20WD9tEF4qE8wjGYRf/7mEP69hTvKEhHdCMOJBGob/lUAYDhxBq4uSrz/VF/MurcLAGDJ/zLw4oY0ruQhImoEw4kEzhVVwCwCnq4qtGujkbocagWCICBhdFe880QfuCgF/HQ4D7Er9kJXwS3viYiuxXAigSs9ddjwz9k8FhWCtVMHwdNVhX1nL+Pxj3fjQkml1GUREclKk8LJkiVLEBERAVdXV0RFRWHHjh03PH/9+vXo06cP3N3dERQUhMmTJ6OoqKhJBTuCzLr+K524jNgpDe7oi6/qtrw/XVCGR5fsxsn8UqnLIiKSDZvDycaNGzFnzhy8+uqrSEtLw4gRIzBmzBhkZ2c3eP7OnTsRFxeHqVOn4ujRo/jqq6+wb98+TJs27baLt1cZBexG7Oy6B3ph0wtD0cW/DfL1VXj8493Yk+m8gZ2I6Go2h5N3330XU6dOxbRp09CjRw8sWrQIoaGhWLp0aYPn79mzBx06dMCsWbMQERGB4cOHY/r06di/f/9tF2+vMgrZjZiAYG83fBU/BAM6tEVplRFxK1KQeDhP6rKIiCRnUziprq5GamoqYmJi6h2PiYnB7t27G3zP0KFDcf78eSQmJkIURVy8eBFff/01xo4d2+jnGAwG6PX6ei9HUdvw78qcE3Ju3u5qrJ06CDF3BKDaZMaMzw/gs91npS6LiEhSNoWTwsJCmEwmBAQE1DseEBCA/Pz8Bt8zdOhQrF+/HuPHj4darUZgYCC8vb3xwQcfNPo5CxcuhFartb5CQ0NtKVPWLpUZUFplhMCGf1TH1UWJpc9G4dnBYRBF4PXNR/Gfn0+yqzEROa0mTYi9doWJKIqNrjo5duwYZs2ahddeew2pqanYsmULsrKyEB8f3+jPnz9/PnQ6nfWVk+M4u2paNl8LbesOVxc2/KNaSoWAvz8ciZdGdwUAfLj1DF77/ig3ayMip6Sy5WQ/Pz8olcrrRkkKCgquG02xWLhwIYYNG4Z58+YBAHr37g0PDw+MGDECb775JoKCgq57j0ajgUbjmPt/cGdYaowgCHjx3i7w9lDjte+PYO2ecyitqsHbT/SBi5Kr/onIedj0N55arUZUVBSSkpLqHU9KSsLQoUMbfE9FRQUUivofo1TWjhg447C1db6JH+ebUMNiB4dj0fi+UCkEfJd+Ac+vS+VuskTkVGz+z7GEhAQsX74cK1euxPHjxzF37lxkZ2dbH9PMnz8fcXFx1vPHjRuHTZs2YenSpcjMzMSuXbswa9YsDBw4EMHBwc13JXYi4xKXEdPNPdy3PT6Ji4JGpcCvxwswaVUKygxGqcsiImoVNj3WAYDx48ejqKgICxYsQF5eHiIjI5GYmIjw8HAAQF5eXr09TyZNmoTS0lJ8+OGHeOmll+Dt7Y177rkH//rXv5rvKuxIJpcR0y26p3sAPpsyENM+2489mcWY8OkerJ48ED4eaqlLIyJqUYJoB89W9Ho9tFotdDodvLy8pC6nyQxGE3r83xaYRSDlL/fC38tV6pLIDhw+r0Pcyr24XFGDLv5tsHbqIARq+WeHiOSvqb+/OcuuFVka/rXRqNDO0zEn/FLz6xWirbfd/eMf70Z2UYXUZRERtRiGk1ZkmQzbqZ0HG/6RTTr7e+Kr+CHo4OuO85crMf6TZGTVPSIkInI0DCetKMO6jJjzTch2oT7u+HL6EHTxb4M8XRWeXJaMMwVsGEhEjofhpBVZV+qwGzE1kb+XKzb8cTC6B3riUqkB45ftwYl8x2nvQEQEMJy0KssGbJ38OXJCTefXRoMNzw1GZHsvFJVX46lP9uBIrk7qsoiImg3DSSup3/CPIyd0e9p6qLF+2mD0CfVGSUUNJny6B+k5JVKXRUTULBhOWklReTX0dQ3/OvgynNDt07q5YN3UgYgObwt9lRHPLt+L/WeLpS6LiOi2MZy0koyC2lGT9t5ubPhHzcbT1QWfTRmIQRE+KDMYEbcyBckZRVKXRUR0WxhOWgl3hqWW4qFRYfXkgRje2Q8V1SZMXp2CXWcKpS6LiKjJGE5aCeebUEtyUyuxfGI0RnZrh6oaM6Z+tg+7MxhQiMg+MZy0Eu5xQi3N1UWJj2OjrgSU1fuxJ5OPeIjI/jCctJKrd4claikalRJLn43CXV3bobLGhMmr9iEli5Nkici+MJy0AoPRhJzLlQA454RanquLEstiozCiix8qa0yYtCoF+7iKh4jsCMNJK8guqoDJLMJDrYQ/G/5RK3B1UeLTuGjrJNlJK1OQeo4BhYjsA8NJK8i4amdYNvyj1mIJKEM7+aK82oSJK/fhQPZlqcsiIrophpNWkFnInjokDTe1EismDsCQjr4oMxgxcUUKd5IlItljOGkFGQVcqUPScVMrsWJSNAZF+KDUYETsir04yIBCRDLGcNIKrCMnXKlDEnFXq7By0gAM7OCD0qragMJmgUQkVwwnLay24R93hyXpeWhUWDV5gLUXT9zKFJy+WCp1WURE12E4aWHF5dXQVdZAEIAIzjkhiXloVFg5eQB6h2hRXF6NZ5bvxdm61gpERHLBcNLCLCt1grVs+Efy4OXqgs8mD0S3AE8UlBrwzPK9OH+5QuqyiIisGE5amHVnWH8+0iH5aOuhxrppg9DRzwO5JZV4dvleFOirpC6LiAgAw0mLs3Qj5jJikpt2nhqsf24QQn3ccLaoAs8s34uiMoPUZRERMZy0tIwC9tQh+QrSuuHzaYMR6OWK0wVliF2RAl1FjdRlEZGTYzhpYZaRE67UIbkK9XHH+ucGwa+NGsfy9Ji4KgVlBqPUZRGRE2M4aUHVRjOyi2snGnIDNpKzTu3aYO3UQfB2d0F6Tgmmrt6HymqT1GURkZNiOGlB2cXl1oZ/AV5s+Efy1iPIC2umDISnRoW9WcWYvi4VBiMDChG1PoaTFmRZRhzRzoMN/8gu9A7xxqrJA+DmosT2U5cwe0M6jCaz1GURkZNhOGlB3BmW7FF0Bx98GhcNtVKBLUfz8ZdvD0MURanLIiInwnDSgix7nHT0Yzgh+zK8ix8WP90XCgH4cv95/DPxOAMKEbUahpMWlHGJDf/Ift0fGYS3Hu0NAPh0RxaW/C9D4oqIyFkwnLQgLiMme/fkgFD8dWwPAMDbP5/Euj3nJK6IiJwBw0kLKS6vRkndZlZs+Ef2bNqIjpg5sjMA4P++P4LNBy9IXBEROTqGkxZieaTT3tsNbmo2/CP79lJMVzw7OAyiCCRsTMf/ThZIXRIROTCGkxaSyfkm5EAEQcCChyLxUJ9gGM0i4telYv/ZYqnLIiIHxXDSQriMmByNQiHgnSf7YGS3dqiqMWPy6n04dkEvdVlE5IAYTloIV+qQI3JRKrDkmSgM6NAWpVVGxK1MQVbdxG8ioubCcNJCLCMn3OOEHI2bWonlEwfgjiAvFJYZ8OzyvcjXVUldFhE5EIaTFlBjutLwr5M/R07I8WjdXPDZlIGI8PNAbkklYlfsRUlFtdRlEZGDYDhpAdnFFTCaRbirlQj0cpW6HKIW0c5Tg7VTByLAS4PTBWWY9tl+djImombBcNICMgpq55tE+LHhHzm2kLbuWDNlELxcVdh/7jJe3HCAjQKJ6LYxnLQA7gxLzqRboCeWTxwAjUqBX48XsFEgEd02hpMWwD1OyNkMjPDBB0/3szYK/M8vJ6UuiYjsGMNJC8iwrNThyAk5kZiegfjnH3oBAD7amoFVu7IkroiI7BXDSQuwjJx04sgJOZmnBobhpdFdAQALfjzGPjxE1CQMJ82suLwal9nwj5zYzHs6I25IOEQReOnLdOw8XSh1SURkZxhOmpll1CRY6wp3tUriaohanyAIeH1cT4ztFYQak4jpa/fjSK5O6rKIyI4wnDSzTM43IYJSIeDd8X0wtJMvyqtNmLQqBWe5zT0R3aImhZMlS5YgIiICrq6uiIqKwo4dO254vsFgwKuvvorw8HBoNBp06tQJK1eubFLBcpdRyPkmRACgUSmxLDaqbpv7asStTEFBKbe5J6KbszmcbNy4EXPmzMGrr76KtLQ0jBgxAmPGjEF2dnaj73nyySfx22+/YcWKFTh58iQ2bNiA7t2731bhcpVRwJETIgtPVxesnjIAYT7uyC6uwKSV+1BaVSN1WUQkc4Jo425JgwYNQv/+/bF06VLrsR49euCRRx7BwoULrzt/y5YteOqpp5CZmQkfH58mFanX66HVaqHT6eDl5dWkn9Fa7nnnf8i8VI61UwdiRJd2UpdDJAtnC8vx+Me7UVhWjSEdfbF6ygBoVEqpyyKiFtbU3982jZxUV1cjNTUVMTEx9Y7HxMRg9+7dDb5n8+bNiI6Oxr///W+0b98eXbt2xZ/+9CdUVlY2+jkGgwF6vb7eyx7UmMzILqpr+MeREyKrDn4eWDVpIDzUSiRnFiHhy4Mwm7mLLBE1zKZwUlhYCJPJhICAgHrHAwICkJ+f3+B7MjMzsXPnThw5cgTffvstFi1ahK+//hozZsxo9HMWLlwIrVZrfYWGhtpSpmRy6hr+ubmw4R/RtXqFaLEsNhouSgE/HcrDPxKPS10SEclUkybEXtvMThTFRhvcmc1mCIKA9evXY+DAgXjggQfw7rvvYvXq1Y2OnsyfPx86nc76ysnJaUqZrc6yM2yEnwcUCjb8I7rW8C5+ePvxPgCAFTuzsHxHpsQVEZEc2RRO/Pz8oFQqrxslKSgouG40xSIoKAjt27eHVqu1HuvRowdEUcT58+cbfI9Go4GXl1e9lz2w7gzrz0c6RI15pF97vHx/7YT4N386jh8PcRdZIqrPpnCiVqsRFRWFpKSkeseTkpIwdOjQBt8zbNgwXLhwAWVlZdZjp06dgkKhQEhISBNKli/rHifcGZbohuLv6oi4IeEAgISNB7Ens0jiiohITmx+rJOQkIDly5dj5cqVOH78OObOnYvs7GzEx8cDqH0kExcXZz1/woQJ8PX1xeTJk3Hs2DFs374d8+bNw5QpU+Dm5tZ8VyIDGexGTHRLLLvI3tczANUmM/64Zj9OXSyVuiwikgmbw8n48eOxaNEiLFiwAH379sX27duRmJiI8PDa/wrKy8urt+dJmzZtkJSUhJKSEkRHR+OZZ57BuHHjsHjx4ua7CpnIrNsBkyt1iG5OqRDw/lP9EB3eFvoqIyauTEG+jpu0EVET9jmRgj3sc3K5vBr9/l77uOvoG/fBQ8O+OkS34nJ5NR77eDcyL5Wje6AnvowfAi9XF6nLIqJm0Cr7nFDjMuu2rQ/SujKYENmgrYcan00eiHaeGpzIL0X82lRUG81Sl0VEEmI4aSYZ1oZ/nG9CZKtQH3esmjQAHmoldmcUYd7X3KSNyJkxnDQTy0odzjchaprI9losfTYKKoWA79Mv4F8/n5C6JCKSCMNJM7HsccJlxERNd2fXdnjrsd4AgGXbMrF6V5bEFRGRFBhOmsmVZcQcOSG6HY9HheBPMV0BAG/8eAz/PZwncUVE1NoYTpqB0WRGdnFtwz/OOSG6fTNGdsYzg8IgisDsjenYd7ZY6pKIqBUxnDSDnMuVqDGJcHVRIFjrWBvLEUlBEAQseDgSo3oEoNpoxrTP9uNMATdpI3IWDCfNIKOg9pFOhF8bNvwjaiZKhYAPnu6HfmHe0FXWYOLKfSjQc5M2ImfAcNIMLHuc8JEOUfNyUyuxYuIARPh5ILekEpNW7UNpVY3UZRFRC2M4aQZcRkzUcnzqNmnza6PGsTw9Xlh/ADUmbtJG5MgYTpqBZaVOJ46cELWIMF93rJw0AO5qJXacLsTL3xyCHXTeIKImYjhpBpaRk45+HDkhaim9Q7zx0YT+UCoEbDqQi3eTTkldEhG1EIaT26SrqEFReTUAzjkhamkju/vjH49EAgA++P0MPt+bfZN3EJE9Yji5TRl1k2EDvdjwj6g1PDUwDLPu7QIA+Ot3h/Hb8YsSV0REzY3h5DZZlhFz1ISo9cwd1QVPRIXALAIzP0/DwZwSqUsiombEcHKbMgvZjZiotQmCgH8+2gt3dm2HyhoTpqzeh3NF5VKXRUTNhOHkNmVaV+pwMixRa3JRKrDkmf7oGeyFovJqTFyZgqIyg9RlEVEzYDi5TRmWlToMJ0Stro1GhVWTBqC9txvOFlVg2pr9qKw2SV0WEd0mhpPbYDSZrUPJHf34WIdICv5ervhsygBo3VyQll2CWV+kwWTmHihE9ozh5Dacr2v4p1Ep0N6bDf+IpNLZ3xPLJ0ZDrVIg6dhF/G3zUW7SRmTHGE5ug2Vn2Ag/Dzb8I5LYgA4+eH98XwgCsHbPOXy8LVPqkoioiRhObgN76hDJy5heQfi/sXcAAP615QS+S8uVuCIiagqGk9tg6UbMnjpE8jFleASmDY8AAMz7+iB2nymUuCIishXDyW3IKOBKHSI5+ssDPTC2dxBqTCKmr03FiXy91CURkQ0YTm6DZeSEG7ARyYtCIeCdJ/pgYIQPSg1GTFq5DxdKKqUui4huEcNJE+kqa1BYVtvwL4LLiIlkx9VFiU9jo9HZvw3y9VWYtCoFusoaqcsiolvAcNJElp1hA7w08HR1kbgaImqI1t0FqycPgL+nBqculmH62v0wGLlJG5HcMZw0kXVnWD/ONyGSs5C27lg1eQA81ErsySzGvK8OwcxN2ohkjeGkiSwjJ5xvQiR/PYO1+Dg2CiqFgM0HL+DfP5+UuiQiugGGkybiHidE9mVEl3b412O9AQAfb8vAmuSz0hZERI1iOGmiDI6cENmdx6JC8KeYrgCA1zcfxZYj+RJXREQNYThpApNZxLmiCgAcOSGyNzNGdsbTA8MgisDsL9KQeu6y1CUR0TUYTprg/OUKVJvM0KgUCGbDPyK7IggC/v5wT9zT3R8GoxnTPttnnUNGRPLAcNIEVzf8U7LhH5HdUSkV+HBCP/QJ0eJyRQ0mrkrBpVKD1GURUR2GkyawTIblfBMi++WuVmHFpAEI83FHTnElpn62D+UGo9RlEREYTpqEe5wQOQa/Nhp8NmUg2rq74NB5HWZ+fgBGk1nqsoicHsNJE1ieT3fy58gJkb2L8PPAikkDoFEpsPXkJfzf90cgitykjUhKDCdNwJETIsfSP6wtPni6HxQCsCElBx/+fkbqkoicGsOJjfRVNSgsq504xzknRI4jpmcg3ng4EgDwTtIpfLU/R+KKiJwXw4mNLJNh/T3Z8I/I0cQODsfzd3cCAMzfdBjbT12SuCIi58RwYqOMAu4MS+TI5sV0wyN9g2E0i3h+XSqO5OqkLonI6TCc2Ciz0BJOON+EyBEpFAL+/XgfDO3ki/JqEyav3ofzlyukLovIqTCc2IgN/4gcn1qlwMexUege6IlLpQZMWrUPJRXVUpdF5DQYTmzEhn9EzsHL1QWrJg9AkNYVZwrK8Mc1qaiqMUldFpFTYDixgcks4qyl4R+XERM5vCCtG1ZPHghPVxVSzhbjpS8PwmzmHihELY3hxAa5lytRbTRDrVKgfVs2/CNyBt0CPbEsNgouSgE/Hc7DPxKPS10SkcNjOLFBRt1k2AhfNvwjciZDO/nhP0/0AQCs2JmF5TsyJa6IyLExnNiAy4iJnNfDfdvjlTHdAQD/SDyOnw7lSVwRkeNqUjhZsmQJIiIi4OrqiqioKOzYseOW3rdr1y6oVCr07du3KR8rucxCdiMmcmbT7+yIiUPCIYrA3C/TkZJVLHVJRA7J5nCyceNGzJkzB6+++irS0tIwYsQIjBkzBtnZ2Td8n06nQ1xcHO69994mFys1a8M/LiMmckqCIOC1cT0Rc0cAqo1mPLdmP84UlEpdFpHDsTmcvPvuu5g6dSqmTZuGHj16YNGiRQgNDcXSpUtv+L7p06djwoQJGDJkSJOLlZq14R/DCZHTUioELH66H/qHeUNXWYOJK/fhor5K6rKIHIpN4aS6uhqpqamIiYmpdzwmJga7d+9u9H2rVq1CRkYGXn/99Vv6HIPBAL1eX+8ltdKqGlwqZcM/IgJcXZRYPnEAIvw8kFtSicmr9qHMYJS6LCKHYVM4KSwshMlkQkBAQL3jAQEByM/Pb/A9p0+fxiuvvIL169dDpVLd0ucsXLgQWq3W+goNDbWlzBZh2Rm2nacGXmz4R+T0fDzU+GzyQPi1UeNYnh7Pr0tFjcksdVlEDqFJE2IFof4yWlEUrzsGACaTCRMmTMAbb7yBrl273vLPnz9/PnQ6nfWVkyN963LrzrB+HDUholphvu5YOWkA3FyU2HG6EK98cxiiyE3aiG7XrQ1l1PHz84NSqbxulKSgoOC60RQAKC0txf79+5GWloaZM2cCAMxmM0RRhEqlwi+//IJ77rnnuvdpNBpoNBpbSmtxmZxvQkQN6B3ijSXP9Me0NfvxzYHzaO/tioSYblKXRWTXbBo5UavViIqKQlJSUr3jSUlJGDp06HXne3l54fDhw0hPT7e+4uPj0a1bN6Snp2PQoEG3V30rsnQj7sT5JkR0jZHd/fHmI5EAgMW/n8GGlBuvXiSiG7Np5AQAEhISEBsbi+joaAwZMgSffPIJsrOzER8fD6D2kUxubi7WrFkDhUKByMjIeu/39/eHq6vrdcflLqOA3YiJqHFPDwxDXkklFv9+Bn/97gj8PTW4t8f1I8pEdHM2h5Px48ejqKgICxYsQF5eHiIjI5GYmIjw8HAAQF5e3k33PLE3JrOIrCJuwEZENzZ3dFdc0FXh69TzmPH5AaybOgjRHXykLovI7giiHcze0uv10Gq10Ol08PLyavXPzymuwIh/b4VaqcDxv9/PvjpE1KgakxnT16bi9xMF8HJV4av4oegW6Cl1WUSSaOrvb/bWuQWWlTod/NwZTIjohlyUCnw0oT+iwttCX2VE3Mq9yCmukLosIrvCcHILrDvD+nG+CRHdnJtaiZUTB6BbgCcu6g2IW5mCwjKD1GUR2Q2Gk1tg6anD+SZEdKu07i74bMpAtPd2Q1ZhOXeRJbIBw8ktsOxxwpU6RGSLQK0r1k4dCB8PNQ7n6jB97X4YjCapyyKSPYaTW5DBkRMiaqKO7dpg9eQB8FArsetMEeZuTIfJLPt1CESSYji5idKqGhRYG/5x5ISIbNc7xBufxEVDrVQg8XA+Xvv+CLe5J7oBhpObyCqsfaTj10YNrRsb/hFR0wzr7If3xveFIADr92Zj0a+npS6JSLYYTm7iyiMdjpoQ0e0Z2zsICx6u3R37/d9OY03yWWkLIpIphpObuDIZlvNNiOj2xQ4Ox5xRXQAAr28+ih8OXpC4IiL5YTi5iUzucUJEzWz2vV0QOzgcoggkfJmOHacvSV0SkawwnNyE5bFOJ3+OnBBR8xAEAX97qCfG9g5CjUnE9LWpSMu+LHVZRLLBcHIDZrNonRDLkRMiak5KhYB3n+yD4Z39UFFtwqRV+3AiXy91WUSywHByA7kllTAYzXBRCghp6yZ1OUTkYDQqJZbFRqFfmDd0lTV4dnkKztb9BxGRM2M4uYHMur8kwn09oFLyVhFR8/PQqLB60kB0D/REYZkBzyzfiwsllVKXRSQp/sa9gYyCuvkmXKlDRC1I6+6CtVMHIcLPA7kllXh2xV42CiSnxnByA5mF3OOEiFpHO08N1k0bhGCtKzIvlSNuRQp0lTVSl0UkCYaTG7iyjJgjJ0TU8tp7u2HdtEHwa6PGsTw9pq7eh4pqdjIm58NwcgNXlhFz5ISIWkfHdm2wZsogeLqqsP/cZUxfm8pOxuR0GE4aUWYw4qK+9plvJy4jJqJWdEewF1ZPHgA3FyV2nC7E7A3pMJrMUpdF1GoYThqRVfdIx9dDDa07G/4RUeuKCvfBp3WdjLcczcfL3xyG2cxOxuQcGE4aYZkM24mTYYlIIsO7+OGDCf2gVAj45sB5LPjxGESRAYUcH8NJIyzLiDtyGTERSei+noF4+/HeAIDVu8/iP7+clLgiopbHcNKIDMu29QwnRCSxR/uH4O8P9wQAfLQ1A4t/Oy1xRUQti+GkEexGTERyEjukA/7yQHcAwLtJp/DxtgyJKyJqOQwnDaht+MdlxEQkL3+8sxP+FNMVAPDWf09gxc4siSsiahkMJw24oKtEVU1tw79QNvwjIhmZeU8XzLqnMwDg7z8ew9o95ySuiKj5MZw0wPJIJ8zHnQ3/iEh25o7uiul3dQQA/N93R7BxX7bEFRE1L/7mbYB1Z1guIyYiGRIEAa/c3x2Th3UAALyy6TC+TTsvbVFEzYjhpAHWybAMJ0QkU4Ig4LUH78Czg8MgisBLXx7Ej4cuSF0WUbNgOGnAlW7EXEZMRPIlCAIWPBSJ8dGhMIvA7C/S8fPRfKnLIrptDCcNyCioHTnhYx0ikjuFQsA/H+2FR/u1h8ksYubnB/D7iYtSl0V0WxhOrlFuMCJfXwUA6MSREyKyA0qFgH8/3htjewehxiQifu0BbD1RIHVZRE3GcHKNrLqdYX081PB2V0tcDRHRrVEpFVg0vi/GRAai2mTG9LWp+O04R1DIPjGcXMOyUqejH0dNiMi+uCgVWPx0PzzQqzagxK9Lxa/HGFDI/jCcXCPjEuebEJH9clEq8P5T/ayPeJ5fn4pfOEmW7AzDyTUyL3GlDhHZNxelAu+P74txfYJRYxLxwvoD2HKEAYXsB8PJNbjHCRE5ApVSgfee7IOH+wbDWLeK57+H86Qui+iWMJxcxWwWrXuccKUOEdk7lVKBd5/siz/0a18bUDak4adDDCgkfyqpC5CTPH0VqmrMUCkEhPq4S10OEdFtUyoE/OeJPhAAbErLxawv0iBCxIO9g6UujahRHDm5imW+SZivO1zY8I+IHIRSIeDtJ/rgsf4hMJlFzP4iHd+n50pdFlGj+Bv4KhkFbPhHRI7JslHbE1G1AWXOxnR8kcJuxiRPDCdXySy0TIblfBMicjxKhYB/Pdbb2izwlU2HsXJnltRlEV2H4eQqlpU6nfw4ckJEjkmhEPD3hyPxxzs7AgAW/HgMH209I3FVRPUxnFyFe5wQkTMQBAHzx3THnFFdAABv/3wS/95yAqIoSlwZUS2GkzoV1UZc0Fka/nHkhIgcmyAImDOqK159oAcAYMn/MvDGD8dgNjOgkPQYTupYHum0dXdBWw82/CMi5/DcnR3x5iORAIDVu89i/qbDMDGgkMQYTupcmQzLURMici7PDg7HO0/0gUIANu7PwZyN6agxmaUui5wYw0mdK8uIOd+EiJzPY1Eh+GhCf7goBfxw8ALi16aistokdVnkpJoUTpYsWYKIiAi4uroiKioKO3bsaPTcTZs2YfTo0WjXrh28vLwwZMgQ/Pzzz00uuKVw5ISInN2YXkH4JDYaGpUCv50oQOyKvdBV1EhdFjkhm8PJxo0bMWfOHLz66qtIS0vDiBEjMGbMGGRnN7yZz/bt2zF69GgkJiYiNTUVI0eOxLhx45CWlnbbxTcn60odP46cEJHzGtndH+umDYKXqwr7z13Gk8uScVFfJXVZ5GQE0ca1Y4MGDUL//v2xdOlS67EePXrgkUcewcKFC2/pZ/Ts2RPjx4/Ha6+9dkvn6/V6aLVa6HQ6eHl52VLuLTGbRfR8/WdU1pjw20t3cbUOETm9E/l6xK1IQUGpAe293bB26kCOLJPNmvr726aRk+rqaqSmpiImJqbe8ZiYGOzevfuWfobZbEZpaSl8fHwaPcdgMECv19d7taR8fRUqa0xQKQSEseEfERG6B3rhm+eHIsLPA7kllXji42QcPq+TuixyEjaFk8LCQphMJgQEBNQ7HhAQgPz8/Fv6Ge+88w7Ky8vx5JNPNnrOwoULodVqra/Q0FBbyrSZZRlxmA8b/hERWYT6uOOr+CHo1V6LovJqPPVJMnadKZS6LHICTfpNLAhCva9FUbzuWEM2bNiAv/3tb9i4cSP8/f0bPW/+/PnQ6XTWV05OTlPKvGWZhdwZloioIX5tNNjwx8EY1tkX5dUmTF61Dz8dypO6LHJwNoUTPz8/KJXK60ZJCgoKrhtNudbGjRsxdepUfPnllxg1atQNz9VoNPDy8qr3aknsRkxE1Lg2GhVWThqAB3oFotpkxswNB7BqFxsGUsuxKZyo1WpERUUhKSmp3vGkpCQMHTq00fdt2LABkyZNwueff46xY8c2rdIWxG7EREQ3plEp8cHT/a0djd/44RgW/HCMu8lSi1DZ+oaEhATExsYiOjoaQ4YMwSeffILs7GzEx8cDqH0kk5ubizVr1gCoDSZxcXF4//33MXjwYOuoi5ubG7RabTNeStNZ5pxwJjoRUeOUdR2NQ9q6463/nsDKXVm4UFKJRU/1hauLUuryyIHYPOdk/PjxWLRoERYsWIC+ffti+/btSExMRHh4OAAgLy+v3p4ny5Ytg9FoxIwZMxAUFGR9zZ49u/mu4jZUVBuRW1IJgI91iIhuRhAExN/VCYuf7ge1UoEtR/Px9Kd7UFRmkLo0ciA273MihZbc5+ToBR3GLt4Jb3cXpL8Wc/M3EBERAGBvZhH+uDYVusoahPu6Y9WkARyBpnpaZZ8TR2R9pMOdYYmIbDKooy++eX4oQn3ccK6oAo8t3Y3Uc8VSl0UOwOnDScYlrtQhImqqzv5tsOn5YegTosXliho8/ele/HDwgtRlkZ1z+nDCybBERLennWftXiijegSg2mjGixvS8G7SKZi5koeaiOGEG7AREd02d7UKy2Kj8NyICADA4t9OY+aGA6ioNkpcGdkjpw4noihaR046MZwQEd0WpULAq2PvwL8f7w0XpYDEw/l44uNk5OkqpS6N7IxTh5N8fRUqqk1QKgSE+TCcEBE1hyejQ/H5c4Ph46HG0Qt6PPThLqRlX5a6LLIjTh1Orm74p1Y59a0gImpWAzr44PsZw9AtwBOXSg0Y/8kefJ+eK3VZZCec+jdyZt1KHS4jJiJqfqE+7vjmhaHWibKzv0jHwv8eh9Fklro0kjmnDicZlvkm/lypQ0TUEtpoVPgkNgrxd3UCACzblomJq1JQXF4tcWUkZ04eTjhyQkTU0hQKAa+M6Y4Pnu4Hd7USu84UYdwHO3HofInUpZFMOfX29aVVNcgqLEeQ1g3tPDXN9nOJiKhhJ/NLEb8uFVmF5VCrFHjz4Ug8OSBU6rKohXD7+ibwdHVB7xBvBhMiolbSLdAT380YhlE9/FFtNOPP3xzCX749DIPRJHVpJCNOHU6IiKj1ad1c8ElsNBJGd4UgAJ/vzcb4ZXusHeKJGE6IiKjVKRQCZt3bBSsnDYCXqwrpOSV44P0dSDp2UerSSAYYToiISDIju/njxxdHoHeIFrrKGjy3Zj/+/uMxVBu53NiZMZwQEZGkwnzd8XX8UEwZVtuXZ8XOLDyxLBk5xRUSV0ZSYTghIiLJqVUKvDbuDnwaFw2tmwsO5pTggcU7sOVIntSlkQQYToiISDZG3xGAn2YNR/8wb5RWGRG/7gD++t1hVFZzNY8zYTghIiJZCWnrjo3Th2D6XR0BAOv2ZGPsBztw+LxO4sqotTCcEBGR7LgoFZg/pgfWTh2IAC8NMi+V4w9LduGjrWdgMst+71C6TQwnREQkWyO6tMPPc+7EA70CYTSLePvnk3jqE06WdXQMJ0REJGve7mp8NKE/3nmiD9poVNh39jLGvL8DX+3PgR10YKEmYDghIiLZEwQBj0WF4L+zRyA6vC3KDEbM+/oQpqzehzwdd5Z1NAwnRERkN0J9aifLvnx/d6iVCmw9eQkx727HhpRsjqI4EIYTIiKyK0qFgOfv7oTE2cPRL8wbpQYj5m86jGdX7OVcFAfBcEJERHaps78nvo4fiv978A64uiiw60wRYt7bjlW7sriix84xnBARkd1SKgRMHR6BLbPvxKAIH1TWmPDGD8fwhyW7cOh8idTlURMxnBARkd3r4OeBDc8Nxt8fiYSnqwqHzuvw8Ee78Nr3R6CrrJG6PLIRwwkRETkEhUJA7OBw/PbSXXikbzBEEViTfA73vrMN36fncsKsHRFEO/inpdfrodVqodPp4OXlJXU5RERkB3afKcRfvz+CzEvlAIChnXzxfw/egR5B/D3SWpr6+5vhhIiIHJbBaMKn2zPxwe9nYDCaoRCA8QPC8FJMV/i10UhdnsNjOCEiImpETnEF3vrvCfx0OA8A4KlRYcY9nTF5WAdoVEqJq3NcDCdEREQ3kZJVjL//eAyHc2s7HIf6uOGV+3tgTGQgFApB4uocD8MJERHRLTCbRWxKy8W/t5xAQakBABDZ3gt/iumGu7q2gyAwpDQXhhMiIiIblBuM+GR7JpbvyER5tQkAMLCDD+bd3w0DOvhIXJ1jYDghIiJqgqIyAz7eloHPks+h2mgGANzdrR3mjuqKPqHe0hZn5xhOiIiIbkOerhIf/H4GG/flWLe/H9HFDy/c3RmDO/rwcU8TMJwQERE1g7OF5Vj8+2l8n37BGlKiwttixshOGNnNnyHFBgwnREREzSinuALLtmfgy/3nrY97egR5YcqwDhjXJxiuLlyCfDMMJ0RERC2gQF+F5TuzsG7POVTUTZz1a6PGhEHheHZwGPw9XSWuUL4YToiIiFrQ5fJqbNiXjbXJ55CnqwIAuCgFjOsdjGeHhKNfqDcf+VyD4YSIiKgV1JjM+PloPlbtOovUc5etx7v4t8H4AaF4tH8IfDzUElYoHwwnRERErexgTgk+Sz6LxMN5qKqpnZfiohQQc0cgHo8OwfDOfnBRKiSuUjoMJ0RERBLRV9Vgc/oFfLk/B4fO66zHfTzUGBMZiHF9gjGwg4/TbZHPcEJERCQDRy/o8OW+HPx4KA9F5dXW4wFeGjzQKwij7wjAgA4+TjGiwnBCREQkI0aTGcmZRfjh4AX890g+SquM1u95uapwVzd/jOrhj7u7+kPr7iJhpS2H4YSIiEimDEYTtp28hJ+PXsTWkwUovmpERakQ0Ku9FkM6+WJoJ19Eh/vATe0Ye6i0ajhZsmQJ3n77beTl5aFnz55YtGgRRowY0ej527ZtQ0JCAo4ePYrg4GD8+c9/Rnx8/C1/HsMJERE5CpNZRHrOZfx6vAC/Hy/AyYul9b7vohTQN9QbUeE+6Bvqjb6h3gjU2udeKq0WTjZu3IjY2FgsWbIEw4YNw7Jly7B8+XIcO3YMYWFh152flZWFyMhIPPfcc5g+fTp27dqFF154ARs2bMBjjz12S5/JcEJERI4qt6QSyRlFda9CXKjbQ+VqAV4a9Anxxh3BXuga4Iku/m3Qwc9D9vNWWi2cDBo0CP3798fSpUutx3r06IFHHnkECxcuvO78l19+GZs3b8bx48etx+Lj43Hw4EEkJyff0mcynBARkTMQRRHZxRVIzijCwfMlSM/R4dTFUmuPn6upFAIi/DwQ4eeB9m3d0N7bDSFt3RHS1g3+Xhq0dVdLHl6a+vtbZcuHVFdXIzU1Fa+88kq94zExMdi9e3eD70lOTkZMTEy9Y/fddx9WrFiBmpoauLhcPwnIYDDAYDBYv9br9baUSUREZJcEQUC4rwfCfT3w1MDapxEV1UYcvaDHwZwSnMwvxamCMpy5WIryahNOF5ThdEFZoz/Py1UFHw812nqo0UajgquLEhqVAhqVEq4uCqgUgnVX28f6h6BXiLZVrvNmbAonhYWFMJlMCAgIqHc8ICAA+fn5Db4nPz+/wfONRiMKCwsRFBR03XsWLlyIN954w5bSiIiIHJK7WoUBHXwwoIOP9Zgoirigq8Kpi6XIKa5A7uVKnC+pxPnLlci9XImicgNEEdBXGaGvMuJsUcVNP6d/eFv7DCcW1/YOEEXxhv0EGjq/oeMW8+fPR0JCgvVrvV6P0NDQppRKRETkcARBQHvv2kc5DTGZRegra1BcUY3L5dUoKq9GucEIg9GMqhqT9X+vflzUxb9Na5V/UzaFEz8/PyiVyutGSQoKCq4bHbEIDAxs8HyVSgVfX98G36PRaKDRaGwpjYiIiOooFQLa1j3OQTupq7GdTTNl1Go1oqKikJSUVO94UlIShg4d2uB7hgwZct35v/zyC6Kjoxucb0JERETOzeZpvAkJCVi+fDlWrlyJ48ePY+7cucjOzrbuWzJ//nzExcVZz4+Pj8e5c+eQkJCA48ePY+XKlVixYgX+9Kc/Nd9VEBERkcOwec7J+PHjUVRUhAULFiAvLw+RkZFITExEeHg4ACAvLw/Z2dnW8yMiIpCYmIi5c+fio48+QnBwMBYvXnzLe5wQERGRc+H29URERNQimvr7W95byxEREZHTYTghIiIiWWE4ISIiIllhOCEiIiJZYTghIiIiWWE4ISIiIllhOCEiIiJZYTghIiIiWWE4ISIiIlmxeft6KVg2sdXr9RJXQkRERLfK8nvb1s3o7SKclJaWAgBCQ0MlroSIiIhsVVpaCq1We8vn20VvHbPZjAsXLsDT0xOCIDTbz9Xr9QgNDUVOTo7T9uzhPeA9AHgPAN4DgPcA4D0AmvceiKKI0tJSBAcHQ6G49ZkkdjFyolAoEBIS0mI/38vLy2n/EFrwHvAeALwHAO8BwHsA8B4AzXcPbBkxseCEWCIiIpIVhhMiIiKSFacOJxqNBq+//jo0Go3UpUiG94D3AOA9AHgPAN4DgPcAkMc9sIsJsUREROQ8nHrkhIiIiOSH4YSIiIhkheGEiIiIZIXhhIiIiGSF4YSIiIhkxanDyZIlSxAREQFXV1dERUVhx44dUpfUJAsXLsSAAQPg6ekJf39/PPLIIzh58mS9c0RRxN/+9jcEBwfDzc0Nd999N44ePVrvHIPBgBdffBF+fn7w8PDAQw89hPPnz9c75/Lly4iNjYVWq4VWq0VsbCxKSkpa+hJtsnDhQgiCgDlz5liPOcv15+bm4tlnn4Wvry/c3d3Rt29fpKamWr/v6PfBaDTir3/9KyIiIuDm5oaOHTtiwYIFMJvN1nMc7R5s374d48aNQ3BwMARBwHfffVfv+615vdnZ2Rg3bhw8PDzg5+eHWbNmobq6uiUuu54b3YOamhq8/PLL6NWrFzw8PBAcHIy4uDhcuHCh3s9w5HtwrenTp0MQBCxatKjecVndA9FJffHFF6KLi4v46aefiseOHRNnz54tenh4iOfOnZO6NJvdd9994qpVq8QjR46I6enp4tixY8WwsDCxrKzMes5bb70lenp6it988414+PBhcfz48WJQUJCo1+ut58THx4vt27cXk5KSxAMHDogjR44U+/TpIxqNRus5999/vxgZGSnu3r1b3L17txgZGSk++OCDrXq9N5KSkiJ26NBB7N27tzh79mzrcWe4/uLiYjE8PFycNGmSuHfvXjErK0v89ddfxTNnzljPcfT78Oabb4q+vr7ijz/+KGZlZYlfffWV2KZNG3HRokXWcxztHiQmJoqvvvqq+M0334gAxG+//bbe91vreo1GoxgZGSmOHDlSPHDggJiUlCQGBweLM2fOlPQelJSUiKNGjRI3btwonjhxQkxOThYHDRokRkVF1fsZjnwPrvbtt9+Kffr0EYODg8X33nuv3vfkdA+cNpwMHDhQjI+Pr3ese/fu4iuvvCJRRc2noKBABCBu27ZNFEVRNJvNYmBgoPjWW29Zz6mqqhK1Wq348ccfi6JY+y+wi4uL+MUXX1jPyc3NFRUKhbhlyxZRFEXx2LFjIgBxz5491nOSk5NFAOKJEyda49JuqLS0VOzSpYuYlJQk3nXXXdZw4izX//LLL4vDhw9v9PvOcB/Gjh0rTpkypd6xRx99VHz22WdFUXT8e3DtL6XWvN7ExERRoVCIubm51nM2bNggajQaUafTtcj1NuRGv5gtUlJSRADW/xh1lntw/vx5sX379uKRI0fE8PDweuFEbvfAKR/rVFdXIzU1FTExMfWOx8TEYPfu3RJV1Xx0Oh0AwMfHBwCQlZWF/Pz8eter0Whw1113Wa83NTUVNTU19c4JDg5GZGSk9Zzk5GRotVoMGjTIes7gwYOh1Wplcd9mzJiBsWPHYtSoUfWOO8v1b968GdHR0XjiiSfg7++Pfv364dNPP7V+3xnuw/Dhw/Hbb7/h1KlTAICDBw9i586deOCBBwA4xz24Wmteb3JyMiIjIxEcHGw957777oPBYKj3aFEOdDodBEGAt7c3AOe4B2azGbGxsZg3bx569ux53ffldg/soitxcyssLITJZEJAQEC94wEBAcjPz5eoquYhiiISEhIwfPhwREZGAoD1mhq63nPnzlnPUavVaNu27XXnWN6fn58Pf3//6z7T399f8vv2xRdf4MCBA9i3b99133OG6weAzMxMLF26FAkJCfjLX/6ClJQUzJo1CxqNBnFxcU5xH15++WXodDp0794dSqUSJpMJ//jHP/D0008DcJ4/Cxateb35+fnXfU7btm2hVqtldU+qqqrwyiuvYMKECdaOu85wD/71r39BpVJh1qxZDX5fbvfAKcOJhSAI9b4WRfG6Y/Zm5syZOHToEHbu3Hnd95pyvdee09D5Ut+3nJwczJ49G7/88gtcXV0bPc9Rr9/CbDYjOjoa//znPwEA/fr1w9GjR7F06VLExcVZz3Pk+7Bx40asW7cOn3/+OXr27In09HTMmTMHwcHBmDhxovU8R74HDWmt65X7PampqcFTTz0Fs9mMJUuW3PR8R7kHqampeP/993HgwAGb65DqHjjlYx0/Pz8olcrrUlxBQcF1ic+evPjii9i8eTO2bt2KkJAQ6/HAwEAAuOH1BgYGorq6GpcvX77hORcvXrzucy9duiTpfUtNTUVBQQGioqKgUqmgUqmwbds2LF68GCqVylqbo16/RVBQEO644456x3r06IHs7GwAjv/nAADmzZuHV155BU899RR69eqF2NhYzJ07FwsXLgTgHPfgaq15vYGBgdd9zuXLl1FTUyOLe1JTU4Mnn3wSWVlZSEpKso6aAI5/D3bs2IGCggKEhYVZ/448d+4cXnrpJXTo0AGA/O6BU4YTtVqNqKgoJCUl1TuelJSEoUOHSlRV04miiJkzZ2LTpk34/fffERERUe/7ERERCAwMrHe91dXV2LZtm/V6o6Ki4OLiUu+cvLw8HDlyxHrOkCFDoNPpkJKSYj1n79690Ol0kt63e++9F4cPH0Z6err1FR0djWeeeQbp6eno2LGjQ1+/xbBhw65bQn7q1CmEh4cDcPw/BwBQUVEBhaL+X2tKpdK6lNgZ7sHVWvN6hwwZgiNHjiAvL896zi+//AKNRoOoqKgWvc6bsQST06dP49dff4Wvr2+97zv6PYiNjcWhQ4fq/R0ZHByMefPm4eeffwYgw3twy1NnHYxlKfGKFSvEY8eOiXPmzBE9PDzEs2fPSl2azZ5//nlRq9WK//vf/8S8vDzrq6KiwnrOW2+9JWq1WnHTpk3i4cOHxaeffrrB5YQhISHir7/+Kh44cEC85557GlxG1rt3bzE5OVlMTk4We/XqJYslpNe6erWOKDrH9aekpIgqlUr8xz/+IZ4+fVpcv3696O7uLq5bt856jqPfh4kTJ4rt27e3LiXetGmT6OfnJ/75z3+2nuNo96C0tFRMS0sT09LSRADiu+++K6alpVlXorTW9VqWkN57773igQMHxF9//VUMCQlplWW0N7oHNTU14kMPPSSGhISI6enp9f6ONBgMTnEPGnLtah1RlNc9cNpwIoqi+NFHH4nh4eGiWq0W+/fvb116a28ANPhatWqV9Ryz2Sy+/vrrYmBgoKjRaMQ777xTPHz4cL2fU1lZKc6cOVP08fER3dzcxAcffFDMzs6ud05RUZH4zDPPiJ6enqKnp6f4zDPPiJcvX26Fq7TNteHEWa7/hx9+ECMjI0WNRiN2795d/OSTT+p939Hvg16vF2fPni2GhYWJrq6uYseOHcVXX3213i8hR7sHW7dubfDf/4kTJ4qi2LrXe+7cOXHs2LGim5ub6OPjI86cOVOsqqpqycsXRfHG9yArK6vRvyO3bt3qFPegIQ2FEzndA0EURfHWx1mIiIiIWpZTzjkhIiIi+WI4ISIiIllhOCEiIiJZYTghIiIiWWE4ISIiIllhOCEiIiJZYTghIiIiWWE4ISIiIllhOCEiIiJZYTghIiIiWWE4ISIiIln5f4LMQx5PDBXVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay']) \n",
    "warmup_alpth = 0.1\n",
    "n = cfg[\"n_epochs\"]*len(train_set)/64\n",
    "\n",
    "def warmup_lr_lambda1(epoch):\n",
    "    if epoch < warmup_alpth*n:\n",
    "        return (epoch+200) / (warmup_alpth * n)\n",
    "    else:\n",
    "        return 1/2*math.cos((epoch-warmup_alpth*n)/n*3.5)+1/2\n",
    "def warmup_lr_lambda2(epoch): \n",
    "    return pow(10,-2*epoch/n)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup_lr_lambda1)\n",
    "li = [warmup_lr_lambda1(i) for i in range(math.floor(n))]\n",
    "plt.plot(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f91df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32223c10c964dc894bb3e02b172ddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasaki\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 046/090 ] loss = 1.12812, acc = 0.21589\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29cfe874c24451281961c5ddf879c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 046/090 ] loss = 1.05711, acc = 0.30641 -> best\n",
      "Best model found at epoch 45, saving model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50788ef76bb4f10b15db686665c0f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 047/090 ] loss = 1.04226, acc = 0.30793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b1e5301ddc4278854db5a50eec3db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 047/090 ] loss = 0.99215, acc = 0.32945 -> best\n",
      "Best model found at epoch 46, saving model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb813569bcaf46dc913dea89b13cc0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 048/090 ] loss = 0.98045, acc = 0.33803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33e10b164b4948b2595cf6522a4491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 048/090 ] loss = 0.95186, acc = 0.33848 -> best\n",
      "Best model found at epoch 47, saving model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28a905c417641b29317408ac9819e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 049/090 ] loss = 0.93776, acc = 0.36469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c88723cf064fee94b30e0889de89b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 049/090 ] loss = 0.92296, acc = 0.36647 -> best\n",
      "Best model found at epoch 48, saving model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7988d58b3b43ad83a49a1db7ebc1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 050/090 ] loss = 0.90374, acc = 0.37959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03efcdc8ac584264b1790970d980e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 050/090 ] loss = 0.91528, acc = 0.36560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696fe7abd5b742f88c6c00877370273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 051/090 ] loss = 0.87750, acc = 0.38993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c441fbf104794b99a4228e64bed21d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m valid_lens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Iterate the validation set by batches.\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(valid_loader):\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# A batch consists of image data and corresponding labels.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     94\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mFoodDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[idx]\n\u001b[0;32m     16\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(fname)\n\u001b[1;32m---> 17\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\transforms\\functional.py:474\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    472\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    473\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:252\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\machine-learning\\env\\lib\\site-packages\\PIL\\Image.py:2082\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2074\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2075\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2076\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2077\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2078\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2079\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2080\u001b[0m         )\n\u001b[1;32m-> 2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize a model, and put it on the device specified.\n",
    "student_model.to(device)\n",
    "teacher_model.to(device) # MEDIUM BASELINE\n",
    "\n",
    "# Initialize trackers, these are not parameters and should not be changed\n",
    "stale = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "t_losses = []\n",
    "t_accs = []\n",
    "\n",
    "v_losses = []\n",
    "v_accs = []\n",
    "\n",
    "teacher_model.eval()  # MEDIUM BASELINE\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    student_model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "    train_lens = []\n",
    "\n",
    "    bar = tqdm(train_loader)\n",
    "    for batch in bar:\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #imgs = imgs.half()\n",
    "        #print(imgs.shape,labels.shape)\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        with torch.no_grad():  # MEDIUM BASELINE\n",
    "            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n",
    "        \n",
    "        logits = student_model(imgs)\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n",
    "#         loss = loss_fn(logits, labels) # SIMPLE BASELINE\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=cfg['grad_norm_max'])\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_batch_len = len(imgs)\n",
    "        train_loss.append(loss.item() * train_batch_len)\n",
    "        train_accs.append(acc)\n",
    "        train_lens.append(train_batch_len)\n",
    "        bar.set_postfix(loss=f'{(sum(train_loss) / sum(train_lens)):.5f}',acc=f\"{(sum(train_accs) / sum(train_lens)).item():.5f}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_loss = sum(train_loss) / sum(train_lens)\n",
    "    train_acc = sum(train_accs) / sum(train_lens)\n",
    "    \n",
    "    t_losses.append(train_loss)\n",
    "    t_accs.append(train_acc)\n",
    "    \n",
    "    \n",
    "    # Print the information.\n",
    "    log(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    student_model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "    valid_lens = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = student_model(imgs)\n",
    "            teacher_logits = teacher_model(imgs) # MEDIUM BASELINE\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n",
    "#         loss = loss_fn(logits, labels) # SIMPLE BASELINE\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        valid_loss.append(loss.item() * batch_len)\n",
    "        valid_accs.append(acc)\n",
    "        valid_lens.append(batch_len)\n",
    "        #break\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / sum(valid_lens)\n",
    "    valid_acc = sum(valid_accs) / sum(valid_lens)\n",
    "    \n",
    "    v_losses.append(valid_loss)\n",
    "    v_accs.append(valid_acc)\n",
    "\n",
    "    # update logs\n",
    "    \n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
    "    else:\n",
    "        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "    # save models\n",
    "    if valid_acc > best_acc:\n",
    "        log(f\"Best model found at epoch {epoch}, saving model\")\n",
    "        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "        best_acc = valid_acc\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale > patience:\n",
    "            log(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "            break\n",
    "log(\"Finish training\")\n",
    "log_fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "for i in range(len(t_accs)):\n",
    "    t_accs[i] = t_accs[i].cpu()\n",
    "for i in range(len(v_accs)):\n",
    "    v_accs[i] = v_accs[i].cpu()    \n",
    "    \n",
    "plt.title(\"acc\")\n",
    "plt.plot(t_accs,'-r')\n",
    "\n",
    "plt.plot(v_accs,'-b')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"loss\")\n",
    "plt.plot(t_losses,'-r')\n",
    "\n",
    "plt.plot(v_losses,'-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for evaluation\n",
    "eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\n",
    "eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from {exp_name}/student_best.ckpt\n",
    "student_model_best = DenseNet(in_channels=3, num_classes=11) # get a new student model to avoid reference before assignment.\n",
    "ckpt_path = f\"{save_path}/student_best.ckpt\" # the ckpt path of the best student model.\n",
    "student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu')) # load the state dict and set it to the student model\n",
    "student_model_best.to(device) # set the student model to device\n",
    "\n",
    "# Start evaluate\n",
    "student_model_best.eval()\n",
    "eval_preds = [] # storing predictions of the evaluation dataset\n",
    "\n",
    "# Iterate the validation set by batches.\n",
    "for batch in tqdm(eval_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    imgs, _ = batch\n",
    "    # We don't need gradient in evaluation.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        logits = student_model_best(imgs.to(device))\n",
    "        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n",
    "    # loss and acc can not be calculated because we do not have the true labels of the evaluation set.\n",
    "    eval_preds += preds\n",
    "\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i))) + str(i)\n",
    "\n",
    "# Save prediction results\n",
    "ids = [pad4(i) for i in range(0,len(eval_set))]\n",
    "categories = eval_preds\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = ids\n",
    "df['Category'] = categories\n",
    "df.to_csv(f\"{save_path}/submission.csv\", index=False) # now you can download the submission.csv and upload it to the kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732128e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
